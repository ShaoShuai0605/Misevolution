import json
import datetime
import os
from typing import Dict, Any, List
import threading


class LLMCallLogger:
    """è®°å½•LLMè°ƒç”¨çš„queryå’Œresponse"""
    
    def __init__(self, log_file_path: str, enabled: bool = True):
        self.log_file_path = log_file_path
        self.enabled = enabled
        self.calls_log: List[Dict[str, Any]] = []
        self.lock = threading.Lock()
        
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(self.log_file_path) if os.path.dirname(self.log_file_path) else ".", exist_ok=True)
    
    def log_call(self, 
                 model: str,
                 query: str, 
                 response: str, 
                 context: Dict[str, Any] = None,
                 error: str = None):
        """è®°å½•ä¸€æ¬¡LLMè°ƒç”¨"""
        if not self.enabled:
            return
            
        call_info = {
            "timestamp": datetime.datetime.now().isoformat(),
            "model": model,
            "query": query,
            "response": response,
            "context": context or {},
            "error": error,
            "query_length": len(query),
            "response_length": len(response) if response else 0
        }
        
        with self.lock:
            self.calls_log.append(call_info)
            # å®æ—¶å†™å…¥æ–‡ä»¶ï¼Œé¿å…ç¨‹åºå´©æºƒæ—¶ä¸¢å¤±æ—¥å¿—
            self._save_to_file()
    
    def _save_to_file(self):
        """ä¿å­˜æ—¥å¿—åˆ°æ–‡ä»¶"""
        try:
            with open(self.log_file_path, 'w', encoding='utf-8') as f:
                json.dump(self.calls_log, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"Warning: Failed to save LLM call log: {e}")
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–è°ƒç”¨ç»Ÿè®¡ä¿¡æ¯"""
        if not self.calls_log:
            return {"total_calls": 0}
            
        total_calls = len(self.calls_log)
        total_query_length = sum(call["query_length"] for call in self.calls_log)
        total_response_length = sum(call["response_length"] for call in self.calls_log)
        errors = sum(1 for call in self.calls_log if call.get("error"))
        
        return {
            "total_calls": total_calls,
            "total_query_length": total_query_length,
            "total_response_length": total_response_length,
            "average_query_length": total_query_length / total_calls if total_calls > 0 else 0,
            "average_response_length": total_response_length / total_calls if total_calls > 0 else 0,
            "error_count": errors,
            "success_rate": (total_calls - errors) / total_calls if total_calls > 0 else 0
        }
    
    def print_stats(self):
        """æ‰“å°è°ƒç”¨ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_stats()
        print(f"\nğŸ“Š LLMè°ƒç”¨ç»Ÿè®¡:")
        print(f"   æ€»è°ƒç”¨æ¬¡æ•°: {stats['total_calls']}")
        print(f"   æˆåŠŸç‡: {stats.get('success_rate', 0):.2%}")
        print(f"   å¹³å‡Queryé•¿åº¦: {stats.get('average_query_length', 0):.0f} å­—ç¬¦")
        print(f"   å¹³å‡Responseé•¿åº¦: {stats.get('average_response_length', 0):.0f} å­—ç¬¦")
        print(f"   é”™è¯¯æ¬¡æ•°: {stats.get('error_count', 0)}")
        print(f"   æ—¥å¿—æ–‡ä»¶: {self.log_file_path}")


# å…¨å±€æ—¥å¿—è®°å½•å™¨å®ä¾‹
_global_logger: LLMCallLogger = None


def init_global_logger(log_file_path: str, enabled: bool = True):
    """åˆå§‹åŒ–å…¨å±€æ—¥å¿—è®°å½•å™¨"""
    global _global_logger
    _global_logger = LLMCallLogger(log_file_path, enabled)
    return _global_logger


def get_global_logger() -> LLMCallLogger:
    """è·å–å…¨å±€æ—¥å¿—è®°å½•å™¨"""
    return _global_logger


def log_llm_call(model: str, query: str, response: str, context: Dict[str, Any] = None, error: str = None):
    """è®°å½•LLMè°ƒç”¨çš„ä¾¿æ·å‡½æ•°"""
    if _global_logger:
        _global_logger.log_call(model, query, response, context, error) 